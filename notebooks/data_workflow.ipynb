{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Data Engineering Workflow\n",
    "\n",
    "This notebook demonstrates a complete data engineering pipeline using:\n",
    "- **dlt (data load tool)**: For data ingestion and schema management\n",
    "- **DuckDB**: As a high-performance analytical database\n",
    "- **Pandas**: For data manipulation and analysis\n",
    "\n",
    "## Workflow Overview\n",
    "1. Load data from CSV using dlt\n",
    "2. Perform data quality checks\n",
    "3. Run analytical queries\n",
    "4. Export results to CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully\n",
      "dlt version: 1.20.0\n",
      "duckdb version: 1.4.3\n",
      "pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import dlt\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(f\"dlt version: {dlt.__version__}\")\n",
    "print(f\"duckdb version: {duckdb.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading with dlt\n",
    "\n",
    "We'll use dlt to load our sample CSV data into DuckDB. dlt automatically:\n",
    "- Infers schema from the data\n",
    "- Creates tables in DuckDB\n",
    "- Handles data type conversions\n",
    "- Manages schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data source function defined\n"
     ]
    }
   ],
   "source": [
    "@dlt.resource(name=\"sales_data\", write_disposition=\"replace\")\n",
    "def load_sales_data():\n",
    "    \"\"\"\n",
    "    Load sales data from CSV file.\n",
    "    \n",
    "    The @dlt.resource decorator marks this function as a data source.\n",
    "    - name: The table name in the database\n",
    "    - write_disposition=\"replace\": Replaces existing data on each run\n",
    "    \"\"\"\n",
    "    # Read CSV file\n",
    "    csv_path = Path(\"../data/sample.csv\")\n",
    "    \n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Sample data not found at {csv_path}\")\n",
    "    \n",
    "    # Load data with pandas for better control\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert to records (list of dicts) for dlt\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    print(f\"✓ Loaded {len(records)} records from {csv_path}\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "print(\"✓ Data source function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pipeline configured\n",
      "  Pipeline name: local_data_pipeline\n",
      "  Destination: DuckDB\n",
      "  Dataset: sales_analytics\n",
      "\n",
      "Loading data into DuckDB...\n",
      "✓ Loaded 10 records from ..\\data\\sample.csv\n",
      "\n",
      "============================================================\n",
      "✓ DATA LOADED SUCCESSFULLY\n",
      "============================================================\n",
      "Loaded at: 2025-12-25 09:51:47.410778+00:00\n",
      "Dataset: sales_analytics\n",
      "Tables created: []\n"
     ]
    }
   ],
   "source": [
    "# Configure and run the dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"local_data_pipeline\",\n",
    "    destination=\"duckdb\",\n",
    "    dataset_name=\"sales_analytics\"\n",
    ")\n",
    "\n",
    "print(\"✓ Pipeline configured\")\n",
    "print(f\"  Pipeline name: local_data_pipeline\")\n",
    "print(f\"  Destination: DuckDB\")\n",
    "print(f\"  Dataset: sales_analytics\")\n",
    "print()\n",
    "\n",
    "# Execute the pipeline\n",
    "print(\"Loading data into DuckDB...\")\n",
    "info = pipeline.run(load_sales_data())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loaded at: {info.started_at}\")\n",
    "print(f\"Dataset: {info.dataset_name}\")\n",
    "print(f\"Tables created: {list(info.load_packages[0].schema_update.keys()) if info.load_packages else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Information:\n",
      "============================================================\n",
      "\n",
      "Table: sales_data\n",
      "Column               Type            Nullable  \n",
      "--------------------------------------------------\n",
      "date                 VARCHAR         YES       \n",
      "product_category     VARCHAR         YES       \n",
      "quantity             BIGINT          YES       \n",
      "price                DOUBLE          YES       \n",
      "region               VARCHAR         YES       \n",
      "customer_id          VARCHAR         YES       \n",
      "_dlt_load_id         VARCHAR         NO        \n",
      "_dlt_id              VARCHAR         NO        \n"
     ]
    }
   ],
   "source": [
    "# Inspect the schema created by dlt\n",
    "print(\"Schema Information:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get the DuckDB connection from dlt\n",
    "with pipeline.sql_client() as client:\n",
    "    # Query to show table structure\n",
    "    with client.execute_query(\"DESCRIBE sales_analytics.sales_data\") as cursor:\n",
    "        schema_info = cursor.fetchall()\n",
    "        \n",
    "        print(f\"\\nTable: sales_data\")\n",
    "        print(f\"{'Column':<20} {'Type':<15} {'Nullable':<10}\")\n",
    "        print(\"-\"*50)\n",
    "        for row in schema_info:\n",
    "            print(f\"{row[0]:<20} {row[1]:<15} {str(row[2]):<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Checks\n",
    "\n",
    "Before analysis, we'll validate data quality by checking for:\n",
    "- Null values\n",
    "- Data type consistency\n",
    "- Business rule violations\n",
    "- Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY CHECKS\n",
      "============================================================\n",
      "\n",
      "1. Checking for NULL values...\n",
      "   ✓ PASS: No NULL values found\n",
      "\n",
      "2. Validating business rules...\n",
      "   ✓ PASS: All business rules satisfied\n",
      "\n",
      "3. Checking for duplicates...\n",
      "   ✓ PASS: No duplicates found\n",
      "\n",
      "4. Data statistics...\n",
      "   Total records: 10\n",
      "   Unique customers: 7\n",
      "   Product categories: 3\n",
      "   Regions: 4\n",
      "   Date range: 2024-01-15 to 2024-01-24\n",
      "\n",
      "============================================================\n",
      "✓ QUALITY CHECKS COMPLETED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def run_quality_checks():\n",
    "    \"\"\"\n",
    "    Execute comprehensive data quality checks on the loaded data.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY CHECKS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    with pipeline.sql_client() as client:\n",
    "        \n",
    "        # Check 1: Null Values\n",
    "        print(\"1. Checking for NULL values...\")\n",
    "        null_check_query = \"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                SUM(CASE WHEN date IS NULL THEN 1 ELSE 0 END) as null_dates,\n",
    "                SUM(CASE WHEN product_category IS NULL THEN 1 ELSE 0 END) as null_categories,\n",
    "                SUM(CASE WHEN quantity IS NULL THEN 1 ELSE 0 END) as null_quantities,\n",
    "                SUM(CASE WHEN price IS NULL THEN 1 ELSE 0 END) as null_prices,\n",
    "                SUM(CASE WHEN region IS NULL THEN 1 ELSE 0 END) as null_regions,\n",
    "                SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_customer_ids\n",
    "            FROM sales_analytics.sales_data\n",
    "        \"\"\"\n",
    "        \n",
    "        with client.execute_query(null_check_query) as cursor:\n",
    "            result = cursor.fetchone()\n",
    "            has_nulls = any(result[1:])  # Check if any null counts > 0\n",
    "            \n",
    "            if has_nulls:\n",
    "                print(\"   ⚠ WARNING: NULL values detected\")\n",
    "                for i, col in enumerate(['dates', 'categories', 'quantities', 'prices', 'regions', 'customer_ids']):\n",
    "                    if result[i+1] > 0:\n",
    "                        print(f\"     - {col}: {result[i+1]} null values\")\n",
    "            else:\n",
    "                print(\"   ✓ PASS: No NULL values found\")\n",
    "        \n",
    "        # Check 2: Business Rule Validation\n",
    "        print(\"\\n2. Validating business rules...\")\n",
    "        business_rule_query = \"\"\"\n",
    "            SELECT \n",
    "                SUM(CASE WHEN quantity <= 0 THEN 1 ELSE 0 END) as invalid_quantity,\n",
    "                SUM(CASE WHEN price <= 0 THEN 1 ELSE 0 END) as invalid_price\n",
    "            FROM sales_analytics.sales_data\n",
    "        \"\"\"\n",
    "        \n",
    "        with client.execute_query(business_rule_query) as cursor:\n",
    "            result = cursor.fetchone()\n",
    "            if result[0] > 0 or result[1] > 0:\n",
    "                print(\"   ⚠ WARNING: Business rule violations detected\")\n",
    "                if result[0] > 0:\n",
    "                    print(f\"     - {result[0]} records with quantity <= 0\")\n",
    "                if result[1] > 0:\n",
    "                    print(f\"     - {result[1]} records with price <= 0\")\n",
    "            else:\n",
    "                print(\"   ✓ PASS: All business rules satisfied\")\n",
    "        \n",
    "        # Check 3: Duplicates\n",
    "        print(\"\\n3. Checking for duplicates...\")\n",
    "        duplicate_query = \"\"\"\n",
    "            SELECT \n",
    "                date, product_category, quantity, price, region, customer_id,\n",
    "                COUNT(*) as duplicate_count\n",
    "            FROM sales_analytics.sales_data\n",
    "            GROUP BY date, product_category, quantity, price, region, customer_id\n",
    "            HAVING COUNT(*) > 1\n",
    "        \"\"\"\n",
    "        \n",
    "        with client.execute_query(duplicate_query) as cursor:\n",
    "            duplicates = cursor.fetchall()\n",
    "            if duplicates:\n",
    "                print(f\"   ⚠ WARNING: {len(duplicates)} duplicate record groups found\")\n",
    "            else:\n",
    "                print(\"   ✓ PASS: No duplicates found\")\n",
    "        \n",
    "        # Check 4: Data Statistics\n",
    "        print(\"\\n4. Data statistics...\")\n",
    "        stats_query = \"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT customer_id) as unique_customers,\n",
    "                COUNT(DISTINCT product_category) as unique_categories,\n",
    "                COUNT(DISTINCT region) as unique_regions,\n",
    "                MIN(date) as earliest_date,\n",
    "                MAX(date) as latest_date\n",
    "            FROM sales_analytics.sales_data\n",
    "        \"\"\"\n",
    "        \n",
    "        with client.execute_query(stats_query) as cursor:\n",
    "            stats = cursor.fetchone()\n",
    "            print(f\"   Total records: {stats[0]}\")\n",
    "            print(f\"   Unique customers: {stats[1]}\")\n",
    "            print(f\"   Product categories: {stats[2]}\")\n",
    "            print(f\"   Regions: {stats[3]}\")\n",
    "            print(f\"   Date range: {stats[4]} to {stats[5]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"✓ QUALITY CHECKS COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the quality checks\n",
    "run_quality_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analytics Queries\n",
    "\n",
    "Now we'll perform various analytical queries to derive insights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Summary Statistics\nprint(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY STATISTICS\")\nprint(\"=\"*60 + \"\\n\")\n\nsummary_query = \"\"\"\n    SELECT \n        COUNT(*) as total_transactions,\n        SUM(quantity * price) as total_revenue,\n        AVG(price) as avg_price,\n        AVG(quantity) as avg_quantity,\n        MIN(date) as earliest_date,\n        MAX(date) as latest_date\n    FROM sales_analytics.sales_data\n\"\"\"\n\nwith pipeline.sql_client() as client:\n    with client.execute_query(summary_query) as cursor:\n        summary_df = cursor.df()\n    \nprint(summary_df.to_string(index=False))\nprint(\"\\n\" + \"=\"*60)\n\n# Store for export\nsummary_stats = summary_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Category Analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"SALES BY PRODUCT CATEGORY\")\nprint(\"=\"*60 + \"\\n\")\n\ncategory_query = \"\"\"\n    SELECT \n        product_category,\n        COUNT(*) as transactions,\n        SUM(quantity) as total_units_sold,\n        SUM(quantity * price) as total_revenue,\n        ROUND(AVG(price), 2) as avg_price,\n        ROUND(AVG(quantity), 2) as avg_quantity\n    FROM sales_analytics.sales_data\n    GROUP BY product_category\n    ORDER BY total_revenue DESC\n\"\"\"\n\nwith pipeline.sql_client() as client:\n    with client.execute_query(category_query) as cursor:\n        category_df = cursor.df()\n\nprint(category_df.to_string(index=False))\nprint(\"\\n\" + \"=\"*60)\n\n# Store for export\ncategory_analysis = category_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Regional Analysis\nprint(\"\\n\" + \"=\"*60)\nprint(\"SALES BY REGION\")\nprint(\"=\"*60 + \"\\n\")\n\nregional_query = \"\"\"\n    SELECT \n        region,\n        COUNT(*) as transactions,\n        SUM(quantity) as total_units_sold,\n        SUM(quantity * price) as total_revenue,\n        ROUND(AVG(quantity * price), 2) as avg_transaction_value\n    FROM sales_analytics.sales_data\n    GROUP BY region\n    ORDER BY total_revenue DESC\n\"\"\"\n\nwith pipeline.sql_client() as client:\n    with client.execute_query(regional_query) as cursor:\n        regional_df = cursor.df()\n\nprint(regional_df.to_string(index=False))\nprint(\"\\n\" + \"=\"*60)\n\n# Store for export\nregional_analysis = regional_df"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results to CSV\n",
    "\n",
    "Finally, we'll export all analysis results to CSV files for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(\"../output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORTING RESULTS\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Add timestamp to filenames for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Export summary statistics\n",
    "summary_file = output_dir / f\"summary_stats_{timestamp}.csv\"\n",
    "summary_stats.to_csv(summary_file, index=False)\n",
    "print(f\"✓ Exported: {summary_file}\")\n",
    "\n",
    "# Export category analysis\n",
    "category_file = output_dir / f\"category_analysis_{timestamp}.csv\"\n",
    "category_analysis.to_csv(category_file, index=False)\n",
    "print(f\"✓ Exported: {category_file}\")\n",
    "\n",
    "# Export regional analysis\n",
    "regional_file = output_dir / f\"regional_analysis_{timestamp}.csv\"\n",
    "regional_analysis.to_csv(regional_file, index=False)\n",
    "print(f\"✓ Exported: {regional_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ ALL RESULTS EXPORTED SUCCESSFULLY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated a complete data engineering workflow:\n",
    "\n",
    "1. ✓ Loaded data from CSV using dlt with automatic schema inference\n",
    "2. ✓ Stored data in DuckDB for high-performance analytics\n",
    "3. ✓ Performed comprehensive data quality checks\n",
    "4. ✓ Executed analytical queries for business insights\n",
    "5. ✓ Exported results to CSV files with timestamps\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "- **dlt** simplifies data ingestion with automatic schema management\n",
    "- **DuckDB** provides SQL analytics without a separate database server\n",
    "- **Data quality checks** are essential before analysis\n",
    "- **Pipeline automation** makes workflows reproducible\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Add more data sources (APIs, databases, etc.)\n",
    "- Implement incremental loading strategies\n",
    "- Create data visualizations with matplotlib or plotly\n",
    "- Schedule the pipeline for regular execution\n",
    "- Add data transformation logic between loading and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local DE Environment",
   "language": "python",
   "name": "local-de-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}